from music21 import converter, instrument, note, chord, stream
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, Activation
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint
import numpy as np

# pulling notes and chords from a midi file
parsed_notes = []
midi = converter.parse('Gold_Silver_Rival_Battle.mid')
# guitar = instrument.partitionByInstrument(midi)
note_to_parse = midi.flat.notes
for x in note_to_parse:
	if isinstance(x, note.Note):
		parsed_notes.append(str(x.pitch))
	elif isinstance(x, chord.Chord):
		parsed_notes.append('.'.join(str(i) for i in x.normalOrder))

# print(parsed_notes)
unique_notes = list(set(parsed_notes))
length_unique_notes = len(unique_notes)

note_to_int = dict((note, position) for position, note in enumerate(unique_notes))
# print(note_to_int)

# print(unique_notes)
input_length = 100
model_input = []
model_output = []

for i in range(0, len(parsed_notes) - input_length, 2):
	sequence_in = parsed_notes[i:i + input_length]
	sequence_out = parsed_notes[i + input_length]
	model_input.append([note_to_int[x] for x in sequence_in])
	model_output.append(note_to_int[sequence_out])
# print(model_input[0:2])
# print(model_output[0:2])

length_model = len(model_input)
# print(length_model)
model_input = np.reshape(model_input, (length_model, input_length, 1))

# normalise input
model_input = model_input/length_unique_notes
# print(model_input)
model_output = np_utils.to_categorical(model_output)
# print(model_output)

# ****** defining the model architecture **********
model = Sequential()
model.add(LSTM( 512, input_shape=(model_input.shape[1], model_input.shape[2]), return_sequences=True ))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(length_unique_notes))
model.add(Activation('sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# creating a checkpoint
checkpoint = ModelCheckpoint("weights_till_now.hd5", monitor='loss', verbose=0, save_best_only=True, mode='min')
callbacks_list = [checkpoint]

# running the model
model.fit(model_input, model_output, epochs = 5, callbacks = callbacks_list)

start = np.random.randint(0, len(model_input)-1)
int_to_note = dict((number, note) for number, note in enumerate(unique_notes))
pattern = list(model_input[start])
prediction_output = []
# generate 500 notes
for note_index in range(50):
	prediction_input = np.reshape(pattern, (1, len(pattern), 1))
	prediction_input = prediction_input / float(length_unique_notes)
	prediction = model.predict(prediction_input, verbose=0)
	index = np.argmax(prediction)
	result = int_to_note[index]
	prediction_output.append(result)
	pattern.append(index)
	pattern = pattern[1:len(pattern)]

offset = 0
output_notes = []
for pattern in prediction_output:
    # pattern is a chord
    if ('.' in pattern) or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes = []
        for current_note in notes_in_chord:
            new_note = note.Note(int(current_note))
            new_note.storedInstrument = instrument.Piano()
            notes.append(new_note)
        new_chord = chord.Chord(notes)
        new_chord.offset = offset
        output_notes.append(new_chord)
    # pattern is a note
    else:
        new_note = note.Note(pattern)
        new_note.offset = offset
        new_note.storedInstrument = instrument.Piano()
        output_notes.append(new_note)
    offset = offset + 0.4

midi_stream = stream.Stream(output_notes)
midi_stream.write('midi', fp='test_output.mid')
